---
title: "String and text analysis"
author: "Eirik Lam√∏y"
date: "11 12 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Recap

[A good blogpost about stringr and rebus](https://rpubs.com/iPhuoc/stringr_manipulation)

```{r}
library(tidyverse);library(babynames);library(rebus)

```


### The packages stringr
Basic/simple ways to seach for pattern/words.
```{r}

boy_names <- babynames::babynames %>% filter(sex == "M") %>% head(n = 20) %>% pull(name)

# Look for excact names 
str_view(boy_names, pattern = "John")

# Look for names with vowels:
str_view(boy_names, pattern = rebus::char_class("aeioAEIOU"))

str_view_all(boy_names, pattern = rebus::char_class("aeioAEIOU"))

# != char_class vowel
str_view(boy_names, pattern = rebus::negated_char_class("aeioAEIOU"))


str_view_all(boy_names,
         pattern = rebus::zero_or_more(rebus::negated_char_class("aeioAEIOU")),
         match = T)

```



### More

Some more example of seach for number/spes.char of different types.


```{r}

strings <- c(
  "apple", 
  "219 733 8965", 
  "329-293-8753", 
  "Work: 579-499-7527; Home: 543.355.3679"
)

str_view_all(strings, pattern = "[0-9]")

str_view_all( strings, pattern = rebus::char_class("[().0-9]") )
```

Seach for words, after three digits

```{r}
three_digits <- DGT %R% DGT %R% DGT 

phone_pattern <- optional(OPEN_PAREN) %R% 
  three_digits %R% 
  zero_or_more(separator) 



string_example <- "345 Eirik"


str_view(string_example, pattern = START %R% three_digits  )


str_view(string_example, pattern =  "[A-Z]" %R% "[a-z]" %R% "[a-z]")



```







## Sentiment analysis


```{r}
library(tidytext)
```

Plutchik's wheel of emotion
```{r}
# As matrix
ag_dtm_m <- as.matrix(ag_dtm)

# Examine line 2206 and columns 245:250
ag_dtm_m[2206, 245:250]

# Tidy up the DTM
ag_tidy <- tidy(ag_dtm)

# Examine tidy with a word you saw
ag_tidy[831:835, ]
```


```{r}
# Subset to AFINN
afinn_lex <- get_sentiments("afinn")

# Count AFINN scores
afinn_lex %>% 
  count(value)

# Subset to nrc
nrc_lex <- get_sentiments("nrc")

# Make the nrc counts object
nrc_counts <- nrc_lex %>% count(sentiment)

# Plot n vs. sentiment
ggplot(nrc_counts, aes(x = fct_reorder(sentiment, n ), y = n)) +
  # Add a col layer
  geom_col() +
  ggthemes::theme_gdocs() +
  coord_flip()

```
 


### Bing lexicion
Lex with pos and neg. word. 
```{r}

# Get Bing lexicon
bing <- get_sentiments("bing")

# Join text to lexicon
ag_bing_words <- inner_join(ag_tidy, bing, by = c("term" = "word"))

# Examine
ag_bing_words

# Get counts by sentiment
ag_bing_words %>%
  count(sentiment)


```

```{r}
moby <-  gutenbergr::gutenberg_download(gutenberg_id = 2701)

head(moby)

# remove blank lines:

moby <- moby %>% filter( !text == "")

moby_clean <- moby %>% select(text) %>% mutate( line = row_number()) %>% select(line, text) %>% mutate( text = stringr::str_trim(text, side = c("both")))

library(tidytext)

tidy_moby <- moby_clean %>% unnest_tokens(word, text)

bing <- get_sentiments("bing")

tidy_moby_lex <- inner_join(tidy_moby, bing, by = c("word" = "word"))


tidy_moby_lex

tidy_moby_lex_count <- tidy_moby_lex %>% count(sentiment, line)


tidy_moby_lex_count %>% 
  pivot_wider( names_from =  sentiment, values_from =  n, values_fill = list(n = 0) )

```

The inner_join -function.


```{r}
# by word
tidy_moby %>% 
  inner_join( bing, by = c("word")) %>% 
  count(word,sentiment ) %>% 
  pivot_wider( values_from = n, names_from = sentiment, values_fill = list(n = 0))
# by index
tidy_moby_polarity<-
  tidy_moby %>% 
  inner_join( bing, by = c("word")) %>% 
  count(sentiment,line ) %>% 
  pivot_wider( values_from = n, names_from = sentiment, values_fill = list(n = 0)) %>% 
  mutate( polarity = positive - negative)
```

```{r}
tidy_moby_polarity %>% 
  ggplot( aes(line, polarity)) +
  geom_smooth( se  = F)

```


## AFINN & NRC inner joins - in more detail.
Afinn lexicon: word ranked between -5 to 5, were minus is negative and plus is positive.

NRC: Word in 10 classes
```{r}
library(textdata)

afinn <- get_sentiments("afinn")

nrc <- get_sentiments("nrc")

tail(afinn)
tail(nrc)
```



```{r}

tidy_moby_afinn <-
  tidy_moby %>% 
  inner_join( afinn, by = c( "word"))


tidy_moby_afinn %>% 
  count(value, line) %>% 
  group_by(line) %>% 
  summarise( total_value_line = sum(value *n)) %>% 
  ggplot( ) +
  aes( line, total_value_line) +
  geom_smooth( se = F)


```


The NRC - without "positive" and "negative" as sentiments word
```{r}
tidy_mody_nrc <- tidy_moby %>%
  #filter( !str_detect(rebus::char_class(rebus::DTG)))
  count( word, line) %>% 
  # Join to nrc lexicon by term = word
  inner_join(nrc, by = c( "word")) %>%
  # Only consider Plutchik sentiments
  filter(!sentiment %in% c("positive", "negative")) %>%
  # Group by sentiment
  group_by(sentiment) %>% 
  # Get total count by sentiment
  summarise(total_count = sum(n))
```

Plot of sentiment words:

```{r}

# Plot total_count vs. sentiment
ggplot(tidy_mody_nrc, aes(x = fct_reorder(sentiment, total_count), y = total_count)) +
  # Add a column geom
  geom_col() +
  coord_flip()
```


##  new section

```{r}
bing <- get_sentiments("bing")

moby_polarity <-
  tidy_moby %>% 
  #filter(! str_detect(word, pattern = "[:digit:]")) %>% 
  #anti_join(stop_words) %>% 
  count(word, line) %>% 
  select( line, word, n) %>% 
  inner_join( bing, by = c("word")) %>% 
  count( sentiment, line) %>% 
  pivot_wider( names_from = sentiment, values_from = "n", values_fill = list(n = 0)) %>% 
  mutate( polarity = positive - negative) %>% 
  mutate( line_number = row_number())
    
moby_polarity %>% 
  ggplot( ) +
  aes( x = line , y = polarity ) +
  geom_smooth( se = F) +
  geom_hline(yintercept = 0, color = "red") +
  ggtitle("Moby Dick Chronological Polarity") +
  ggthemes::theme_gdocs()


#tidytext::stop_words
```



## Frequency analysis




```{r}
moby_tidy_sentiment <- 
  tidy_moby %>% 
  inner_join( bing, by = c("word")) %>% 
  count( word, sentiment ) %>% 
  pivot_wider( names_from = sentiment, values_from = n, values_fill = list( n = 0)) %>% 
  mutate( polarity = positive - negative)

moby_tidy_sentiment

```

One of the easiest ways to explore data is with a frequency analysis. Although not difficult, in sentiment analysis this simple method can be surprisingly illuminating. Specifically, you will build a barplot. In this exercise you are once again working with moby and bing to construct your visual.

```{r}

# Filter for word abov. 50
moby_tidy_pol <-
  moby_tidy_sentiment %>% 
  filter( abs(polarity) > 50) %>% 
  mutate( pos_or_neg = ifelse( polarity > 0, "positive", "negative"))


moby_tidy_pol %>% 
  ggplot( ) +
  aes(  x = fct_reorder(word, polarity) , polarity, fill =pos_or_neg ) +
  geom_col() +
  ggtitle("Moby Dick: Sentiment Word Frequency") + 
  ggthemes::theme_gdocs() +
  # Rotate text and vertically justify
  theme(axis.text.x = element_text(angle = 90, vjust = -0.1)) +
  coord_flip()
```



## Introsepction using sentiment analysis

comparing frequent words in Plutchiks Framework.

Where and how often appear Moby?

How to detect spes. word:
```{r}
x <- c( "Eirik", "Moby","Trym")

grep("Moby", x)

grepl("Moby", x)

# str_count("Moby", x)

str_detect(x, "Moby")

str_detect(x, "Moby")

!str_detect(x, "Moby")

# fins:
str_detect(x, "Eirik|Moby")

# NOot
!str_detect(x, "Eirik|Moby")



```


To compar polarity mixture: Stacked comparisons:

```{r}
mtcars %>% 
  ggplot() +
  aes( x = cyl , y  = mpg) + geom_point() + annotate(geom = "Text", x = 6, y = 20 , label = "Text")


```


### 

```{r}
pos_terms <- bos_reviews %>%
  # Add polarity column
  mutate(polarity = bos_pol$all$polarity) %>%
  # Filter for negative polarity
  filter(polarity > 0) %>%
  # Extract comments column
  pull(comments) %>% 
  # Paste and collapse
  paste(collapse = " ")

neg_terms <- bos_reviews %>%
  # Add polarity column
  mutate(polarity = bos_pol$all$polarity) %>%
  # Filter for negative polarity
  filter(polarity < 0) %>%
  # Extract comments column
  pull(comments) %>% 
  # Paste and collapse
  paste(collapse = " ")

# Concatenate the terms
all_corpus <- c(pos_terms, neg_terms) %>% 
  # Source from a vector
  VectorSource() %>% 
  # Create a volatile corpus
  VCorpus()


all_tdm <- TermDocumentMatrix(
  # Use all_corpus
  all_corpus, 
  control = list(
    # Use TFIDF weighting
    weighting = weightTfIdf, 
    # Remove the punctuation
    removePunctuation = TRUE,
    # Use English stopwords
    stopwords = stopwords(kind = "en")
```




